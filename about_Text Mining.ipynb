{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3. Text Mining 강의내용 정리\n",
    "   \n",
    "### 201821479 황혜린\n",
    "\n",
    "\n",
    "                           \n",
    "\n",
    "---\n",
    "참고: 캡스톤 3주차 text mining강의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < 목차 >   \n",
    "\n",
    "1. 텍스트 마이닝의 이해 \n",
    " - 텍스트 마이닝이란?\n",
    " \n",
    "2. 텍스트 마이닝 방법론: 도구 및 원리의 이해\n",
    " - 텍스트 마이닝 방법\n",
    " - 텍스트 마이닝 도구 - 파이썬\n",
    " - 텍스트 마이닝 기본도구\n",
    "    \n",
    "3. 텍스트 마이닝의 문제점\n",
    " - 차원의 저주\n",
    " - 단어 빈도의 불균형\n",
    " - 단어가 쓰인 순서정보의 손실  \n",
    "     \n",
    "4. 문제 해결을 위한 방법:     \n",
    " - 차원의 저주 해결방법\n",
    " - 단어 빈도의 불균형 해결방법\n",
    " - 단어가 쓰인 순서정보의 손실 해결방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 텍스트 마이닝의 이해\n",
    "### - 텍스트 마이닝이란?\n",
    "\n",
    "텍스트 마이닝이란, 텍스트를 분석을 위한 데이터형태로 변환하여 가치와 의미있는 정보를 얻는 것이다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트 마이닝 방법론: 도구 및 원리의 이해\n",
    "### - 텍스트 마이닝 방법\n",
    "텍스트 마이닝을 하는 도구는, 기본적으로 NLP(자연어처리)가 있다. NLP에 대해서는 뒤에 자세히 설명되어있다. NLP를 통해 기계가 이해할 수 있는 언어로 텍스트를 변형한다. 변형된 언어를 사용하여 머신러닝으로 데이터를 분석한다. 머신러닝은 최근에 연구가 활발히 진행되면서 발전하고 있는 분야이다. \n",
    "\n",
    "### - 텍스트 마이닝 도구 - 파이썬\n",
    "Scikit Learn, Keras등이 현재 많이 쓰인다. 머신러닝, 딥러닝을 사용하는 라이브러리이다. \n",
    "\n",
    "### - 텍스트 마이닝 기본도구\n",
    "- NLP\n",
    "\n",
    "위에 설명한 NLP에 대한 부연설명이다. NLP의 과정에 대한 설명을 하기 이전에 우리가 이런 과정을 거치는 목적은 document, sentence등을 sparse vector로 변환하기 위함이다.                   \n",
    "NLP과정은 텍스트마이닝 대상이 되는 document에서 문장들을 단어단위로 잘게 쪼개, 단어가 가진 여려 형태를 (ex, 학교에서, 학교에,,등등) 하나의 형태로 통일시키는 표준화 과정을 거친다. 나누어진 단어들에게는 품사를 부착한다. (각각의 과정에 대한 명칭이 있지만 큰 틀을 이해하기 위해 이번 강의요약에서는 생략하였습니다.)                      \n",
    "  \n",
    "\n",
    "- 머신러닝   \n",
    "\n",
    "나이브 베이즈: 베이즈 정리를 활용한다. 나이브 베이즈는 스팸 메일 필터, 텍스트 분류, 감정 분석, 추천 시스템 등에 광범위하게 활용되는 분류 기법이다.    \n",
    "\n",
    "\n",
    "로지스틱 회귀: 회귀를 사용한다. 데이터가 어떤 범주에 속할 확률을 0에서 1 사이의 값으로 예측하고, 임계값을 기준으로 더 작으면0, 크면 1의 범주에 속하는 것으로 분류해주는 지도 학습 알고리즘이다.         \n",
    "\n",
    "\n",
    "릿지 회귀: 규제가 추가된 선형 회귀이다. 목적함수에 parameter에 대한 규제항(L2 norm)를 추가하여 모형의 과적합을 방지한다.  \n",
    "\n",
    "\n",
    "라쏘 회귀: L1 norm을 규제항으로 사용함으로써 0에 가까운 계수를 0으로 만들어 영향을 거의 미치지 않는 단어들을 제외한다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 텍스트 마이닝의 문제점\n",
    "\n",
    "### - 차원의 저주\n",
    "각 데이터 간의 거리가 너무 멀게 위치해서 생기는 문제이다. 차원축소방법을 사용해서 이를 해결할 수 있다. 해결방법은 뒤에 자세히 설명하도록 하겠다. \n",
    "\n",
    "### - 단어 빈도의 불균형\n",
    "단어의 빈도의 불균형 문제란, 단어의 빈도가 불균형해서 극히 소수의 빈도수가 높은 단어들만 분석에 영향을 미치게 되는 것이다. 차원의 저주에서와 마찬가지로 해결방법은 위에서 설명하겠다. \n",
    "\n",
    "### - 단어가 쓰인 순서정보의 손실\n",
    "텍스트 마이닝을 할 때, 문맥을 고려하지 않고 단어의 빈도수만 가지고 문서의 의미를 파악하여 생기는 문제이다. 번역과 같은 sequence to sequence에서 매우 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 문제 해결을 위한 방법\n",
    "\n",
    "### - 차원의 저주에 대한 문제 해결 방법\n",
    "\n",
    "- feature extraction\n",
    "\n",
    "데이터의 정보를 뽑아내서 데이터 중에서 몇개만 추리는 것이다. 데이터를 2차원 그래프상에 대응시키고 1차원으로 차원을 축소시킨다. 이 때, 2차원의 점들을 1차원의 직선 위에 수직으로 대응시킨다. 데이터의 분산을 최대로 보존하기 위하여 점들을 최대한 멀리 흩어지게 하는 1차원 직선을 긋는다. \n",
    "\n",
    "- embedding\n",
    "\n",
    "단어를 one-hot-encoding으로 표현하여 단어에 대한 벡터의 차원을 축소한다. one-hot-encoding이란 각 문서에서 사용된 모든 단어들의 수만큼의 길이를 가지는 벡터로 표현한 것이다. (심한 경우 길이 30만의 벡터 중에서 하나만 1인 sparse vector가 된다.)\n",
    "\n",
    "\n",
    "### - 단어 빈도의 불균형 해결\n",
    "- feature selection\n",
    "\n",
    "데이터 중에서 몇개만 고르고 나머지는 버리는 것이다. \n",
    "\n",
    "여기에 추가로 위의 'feature extraction'방법을 사용하여 문제를 해결한다. \n",
    "\n",
    "\n",
    "\n",
    "### - 단어가 쓰인 순서정보의 손실 해결\n",
    "\n",
    "- n-gram\n",
    "\n",
    "문장이 있을 때, 단어 하나씩 쪼개는 uni-gram에 단어를 두개씩 쪼개는 bi-gram, 세개씩 쪼개는 tri-gram을 추가하면서 feature의 수를 증가시켜 사용한다. 그러나 문맥 파악악에는 유리하지만 차원이 더욱 증가한다는 단점이 있다. \n",
    "\n",
    "- Deep learning\n",
    "\n",
    "RNN: 앞 RNN Cell의 정보를 다음 RNN Cell에게 전달하는 역할을 하는 hidden node가 존재하여 sequence를 고려하는 텍스트 마이닝을 할 수 있다. 하지만 기울기 소실 현상으로 시간의 길어질수록 오래전 Data의 영향력이 점점 약해진다는 단점이 있다. \n",
    "\n",
    "LSTM: RNN통로와 더불어 직통통로를 만들어 RNN의 문제를 해결한다. \n",
    "\n",
    "Attention: 주로 변역에 사용되는 Sequence-to-sequence를 해결하기 위한 방법이다. 번역은 출력값 또한 입력값처럼 sequence를 가지므로, 입력된 단어들로부터 출력단어에 직접 링크를 만드는 attention방법을 통해 해결한다. \n",
    "\n",
    "Transformer: 입력한 단어들끼리도 상호연관성이 있는 것에 착안하여 입력->출력으로의 attention 이외에, 입력+출력 -> 출력으로의 attention을 추가한다. encoder와 decoder가 서로 다른 attention구조를 사용한다.\n",
    "\n",
    "BERT: 양방향 transformer 인코더를 사용한다. transfer learning에서 feature + model을 함께 transfer하고 fine tuning을 통해서 적용한 방식을 선택한다. 거의 모든 분야에서 높은 점수를 기록하고 있다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
